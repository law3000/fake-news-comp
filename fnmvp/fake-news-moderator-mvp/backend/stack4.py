# -*- coding: utf-8 -*-
"""stack4

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17miL4qwnVTszm1v3BgcNMlk2Lmw-g1YV

Stacked Fake‑News Detection Ensemble (Gradient‑Boosted Meta‑Learner)
===================================================================
End‑to‑end training script for a four‑branch stacked ensemble that classifies
news articles as *fake* or *real* and now uses **GradientBoostingClassifier** as
the meta‑learner.

Branches
--------
• **ContentModel**   – BERT/DistilBERT fine‑tuned on article text
• **ContextModel**   – LightGBM over propagation / user features
• **KnowledgeCheckerModel** – evidence retrieval + NLI score (placeholder)
• **VisionLanguageModel** – CLIP similarity between text & accompanying image
• **Meta‑Learner**   – GradientBoostingClassifier combining the four scores

Quick start
-----------
"""

"""
Stacked Fake‑News Detection Ensemble (Gradient‑Boosted Meta‑Learner)
===================================================================
End‑to‑end training script for a four‑branch stacked ensemble that classifies
news articles as *fake* or *real* and now uses **GradientBoostingClassifier** as
the meta‑learner.

Branches
--------
• **ContentModel**   – BERT/DistilBERT fine‑tuned on article text
• **ContextModel**   – LightGBM over propagation / user features
• **KnowledgeCheckerModel** – evidence retrieval + NLI score (placeholder)
• **VisionLanguageModel** – CLIP similarity between text & accompanying image
• **Meta‑Learner**   – GradientBoostingClassifier combining the four scores

Quick start
-----------
```bash
pip install transformers torch datasets scikit‑learn pandas numpy pillow tqdm \
            sentencepiece timm ftfy regex joblib networkx sentence‑transformers lightgbm

python stacked_fake_news_detector.py \
  --train_csv data/train.csv \
  --test_csv  data/test.csv  \
  --output    results.csv
```
"""

import argparse
import json
from pathlib import Path
from typing import List, Optional, Tuple
import warnings

import numpy as np
import pandas as pd
from PIL import Image
from tqdm.auto import tqdm

from sklearn.model_selection import StratifiedKFold
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import classification_report, roc_auc_score, accuracy_score
from sklearn.preprocessing import StandardScaler
import joblib

import torch
from torch.utils.data import Dataset, DataLoader
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    pipeline as hf_pipeline,
)

# Suppress warnings for cleaner output
warnings.filterwarnings("ignore", category=FutureWarning)

# Global fast mode flag (set in main())
FAST_MODE: bool = False

# ---------------------------------------------------------------------------
# DATASET
# ---------------------------------------------------------------------------
class NewsSample:
    """Container for multimodal fields."""

    def __init__(self, text: str, image_path: Optional[Path], context_json: str):
        self.text = text
        self.image_path = image_path  # may be None
        self.context_json = context_json  # JSON string of social context stats


class NewsDataset(Dataset):
    def __init__(self, df: pd.DataFrame, img_dir: Optional[Path] = None):
        self.samples: List[NewsSample] = []
        for _, row in df.iterrows():
            img_path = None
            if img_dir and pd.notna(row.get("image_filename")) and str(row.get("image_filename")).strip():
                img_path = (img_dir / str(row["image_filename"]).strip())

            self.samples.append(
                NewsSample(
                    text=str(row["text"]) if pd.notna(row["text"]) else "",
                    image_path=img_path,
                    context_json=str(row.get("context", "{}")),
                )
            )

        # Handle labels - may not exist in test set
        if "label" in df.columns:
            self.labels = df["label"].values.astype(int)
        else:
            self.labels = np.zeros(len(df))  # dummy labels for test set

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        return self.samples[idx], self.labels[idx]


# ---------------------------------------------------------------------------
# BASE LEARNER 1 — CONTENT (TEXT ONLY)
# ---------------------------------------------------------------------------
class ContentModel:
    def __init__(self, model_name="distilbert-base-uncased", device=None):
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        self.model_name = model_name
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForSequenceClassification.from_pretrained(
            model_name, num_labels=2
        ).to(self.device)

    def fit(self, texts: List[str], labels: np.ndarray, epochs: int = 1, batch: int = 8):
        from torch.optim import AdamW
        from torch.nn import CrossEntropyLoss

        self.model.train()
        optim = AdamW(self.model.parameters(), lr=2e-5)

        # Filter out empty texts
        valid_data = [(t, l) for t, l in zip(texts, labels) if t.strip()]
        if not valid_data:
            print("Warning: No valid texts for ContentModel training")
            return

        loader = DataLoader(valid_data, batch_size=batch, shuffle=True)

        for epoch in range(epochs):
            total_loss = 0
            for x, y in tqdm(loader, desc=f"Content epoch {epoch+1}", leave=False):
                try:
                    enc = self.tokenizer(
                        list(x),
                        return_tensors="pt",
                        padding=True,
                        truncation=True,
                        max_length=(256 if FAST_MODE else 512)
                    ).to(self.device)

                    labels_tensor = torch.tensor(list(y), dtype=torch.long).to(self.device)
                    outputs = self.model(**enc, labels=labels_tensor)
                    loss = outputs.loss

                    loss.backward()
                    optim.step()
                    optim.zero_grad()
                    total_loss += loss.item()
                except Exception as e:
                    print(f"Error in ContentModel training batch: {e}")
                    continue

            print(f"Content model epoch {epoch+1} avg loss: {total_loss/len(loader):.4f}")

    @torch.inference_mode()
    def predict_proba(self, texts: List[str], batch: int = 16) -> np.ndarray:
        if not texts:
            return np.array([])

        self.model.eval()
        out = []

        for i in range(0, len(texts), batch):
            batch_texts = texts[i:i+batch]
            # Handle empty texts
            batch_texts = [t if t.strip() else "no content" for t in batch_texts]

            try:
                enc = self.tokenizer(
                    batch_texts,
                    return_tensors="pt",
                    padding=True,
                    truncation=True,
                    max_length=(256 if FAST_MODE else 512)
                ).to(self.device)

                logits = self.model(**enc).logits
                probs = torch.softmax(logits, dim=1)[:, 1].cpu().numpy()
                out.append(probs)
            except Exception as e:
                print(f"Error in ContentModel prediction batch: {e}")
                # Return default probabilities for failed batch
                out.append(np.full(len(batch_texts), 0.5))

        return np.concatenate(out)


# ---------------------------------------------------------------------------
# BASE LEARNER 2 — CONTEXT (PROPAGATION / USER)
# ---------------------------------------------------------------------------
class ContextModel:
    def __init__(self):
        try:
            from lightgbm import LGBMClassifier
            self.model = LGBMClassifier(
                n_estimators=400,
                learning_rate=0.05,
                random_state=42,
                verbose=-1
            )
        except ImportError:
            print("LightGBM not available, using RandomForest")
            from sklearn.ensemble import RandomForestClassifier
            self.model = RandomForestClassifier(n_estimators=100, random_state=42)

    @staticmethod
    def _extract_features(ctx_jsons: List[str]) -> np.ndarray:
        feats = []
        for s in ctx_jsons:
            try:
                d = json.loads(s) if s.strip() else {}
            except json.JSONDecodeError:
                d = {}

            feats.append([
                d.get("share_count", 0),
                d.get("unique_users", 0),
                d.get("avg_followers", 0),
                d.get("burstiness", 0.0),
                d.get("sentiment_score", 0.0),
                d.get("engagement_rate", 0.0),
            ])
        return np.asarray(feats, dtype=float)

    def fit(self, ctx_jsons: List[str], y: np.ndarray):
        features = self._extract_features(ctx_jsons)
        self.scaler = StandardScaler()
        features_scaled = self.scaler.fit_transform(features)
        self.model.fit(features_scaled, y)

    def predict_proba(self, ctx_jsons: List[str]) -> np.ndarray:
        if not hasattr(self, 'scaler'):
            # Model not trained, return default probabilities
            return np.full(len(ctx_jsons), 0.5)

        features = self._extract_features(ctx_jsons)
        features_scaled = self.scaler.transform(features)
        return self.model.predict_proba(features_scaled)[:, 1]


# ---------------------------------------------------------------------------
# BASE LEARNER 3 — KNOWLEDGE / NLI (PLACEHOLDER)
# ---------------------------------------------------------------------------
class KnowledgeCheckerModel:
    def __init__(self, device=None):
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        try:
            device_id = 0 if self.device == "cuda" else -1
            self.nli = hf_pipeline(
                "text-classification",
                model="facebook/bart-large-mnli",
                device=device_id,
                return_all_scores=True
            )
        except Exception as e:
            print(f"Failed to load NLI model: {e}")
            self.nli = None

    def _retrieve_evidence(self, claim: str) -> str:
        # TODO: implement real retrieval / RAG
        # This is a placeholder - in practice, you would:
        # 1. Query knowledge base/Wikipedia/fact-checking sites
        # 2. Return relevant evidence passages
        return "No evidence retrieved"

    def _calculate_nli_score(self, premise: str, hypothesis: str) -> float:
        if self.nli is None:
            return 0.5  # Default score if model not available

        try:
            result = self.nli(f"{premise} [SEP] {hypothesis}")

            # Find entailment score
            entailment_score = 0.33  # default
            for score_dict in result:
                if score_dict['label'] == 'ENTAILMENT':
                    entailment_score = score_dict['score']
                    break

            # Convert to fake news probability (lower entailment = higher fake probability)
            return 1 - entailment_score
        except Exception as e:
            print(f"Error in NLI scoring: {e}")
            return 0.5

    def fit(self, *args, **kwargs):
        # This model doesn't require training
        return self

    def predict_proba(self, texts: List[str]) -> np.ndarray:
        scores = []
        for text in tqdm(texts, desc="NLI scoring", leave=False):
            if not text.strip():
                scores.append(0.5)
                continue

            evidence = self._retrieve_evidence(text)
            score = self._calculate_nli_score(evidence, text)
            scores.append(score)

        return np.array(scores)


# ---------------------------------------------------------------------------
# BASE LEARNER 4 — VISION–LANGUAGE (CLIP)
# ---------------------------------------------------------------------------
class VisionLanguageModel:
    def __init__(self, device=None):
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        try:
            self.clip = CLIPModel.from_pretrained("openai/clip-vit-base-patch32").to(self.device)
            self.clip.eval()
            self.processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
        except Exception as e:
            print(f"Failed to load CLIP model: {e}")
            self.clip = None
            self.processor = None

        self.w0, self.w1 = 0.0, -4.0  # logistic regression parameters

    def _compute_similarity(self, texts: List[str], images: List[Image.Image]) -> np.ndarray:
        if self.clip is None:
            return np.full(len(texts), 0.5)

        try:
            inputs = self.processor(
                text=texts,
                images=images,
                return_tensors="pt",
                padding=True
            ).to(self.device)

            with torch.no_grad():
                outputs = self.clip(**inputs)
                logits_per_image = outputs.logits_per_image
                similarities = logits_per_image.diag().cpu().numpy()

            # Normalize similarities to [0, 1]
            if len(similarities) > 1:
                sim_min, sim_max = similarities.min(), similarities.max()
                if sim_max > sim_min:
                    similarities = (similarities - sim_min) / (sim_max - sim_min)

            return similarities
        except Exception as e:
            print(f"Error computing CLIP similarity: {e}")
            return np.full(len(texts), 0.5)

    def fit(self, texts: List[str], image_paths: List[Optional[Path]], y: np.ndarray):
        # Load images
        images = []
        for path in image_paths:
            if path and path.exists():
                try:
                    img = Image.open(path).convert("RGB")
                    images.append(img)
                except Exception:
                    images.append(Image.new("RGB", (224, 224)))  # default image
            else:
                images.append(Image.new("RGB", (224, 224)))  # default image

        # Compute similarities
        similarities = self.predict_similarity(texts, image_paths)

        # Fit logistic regression to map similarities to fake news probability
        try:
            from sklearn.linear_model import LogisticRegression
            lr = LogisticRegression(random_state=42)
            lr.fit(similarities.reshape(-1, 1), y)
            self.w0, self.w1 = lr.intercept_[0], lr.coef_[0][0]
        except Exception as e:
            print(f"Error fitting vision-language model: {e}")
            self.w0, self.w1 = 0.0, -1.0

    def predict_similarity(self, texts: List[str], image_paths: List[Optional[Path]]) -> np.ndarray:
        # Load images
        images = []
        for path in image_paths:
            if path and path.exists():
                try:
                    img = Image.open(path).convert("RGB")
                    images.append(img)
                except Exception:
                    images.append(Image.new("RGB", (224, 224)))
            else:
                images.append(Image.new("RGB", (224, 224)))

        # Compute similarities in batches
        similarities = []
        batch_size = 8
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i+batch_size]
            batch_images = images[i:i+batch_size]
            batch_sims = self._compute_similarity(batch_texts, batch_images)
            similarities.extend(batch_sims)

        return np.array(similarities)

    def predict_proba(self, texts: List[str], image_paths: List[Optional[Path]]) -> np.ndarray:
        similarities = self.predict_similarity(texts, image_paths)

        # Apply logistic transformation
        logits = self.w0 + self.w1 * similarities
        probabilities = 1 / (1 + np.exp(-np.clip(logits, -500, 500)))  # Clip to avoid overflow

        return probabilities


# ---------------------------------------------------------------------------
# STACKING UTILITIES
# ---------------------------------------------------------------------------

def generate_oof_predictions(models: List, dataset: NewsDataset, n_folds: int = 5) -> np.ndarray:
    """Generate out-of-fold predictions for stacking."""
    kf = StratifiedKFold(n_folds, shuffle=True, random_state=42)
    oof_preds = np.zeros((len(dataset), len(models)))
    labels = dataset.labels

    # Extract data for each model type
    texts = [sample.text for sample, _ in dataset]
    context_jsons = [sample.context_json for sample, _ in dataset]
    image_paths = [sample.image_path for sample, _ in dataset]

    for fold_idx, (train_idx, val_idx) in enumerate(kf.split(np.zeros(len(labels)), labels), 1):
        print(f"\nProcessing Fold {fold_idx}/{n_folds}")

        # Split data for this fold
        train_texts = [texts[i] for i in train_idx]
        val_texts = [texts[i] for i in val_idx]
        train_contexts = [context_jsons[i] for i in train_idx]
        val_contexts = [context_jsons[i] for i in val_idx]
        train_images = [image_paths[i] for i in train_idx]
        val_images = [image_paths[i] for i in val_idx]
        train_labels = labels[train_idx]

        # Train and predict with each model
        for model_idx, model in enumerate(models):
            model_name = model.__class__.__name__
            print(f"  Training {model_name}...")

            try:
                if isinstance(model, ContentModel):
                    model.fit(train_texts, train_labels, epochs=1)
                    oof_preds[val_idx, model_idx] = model.predict_proba(val_texts)
                elif isinstance(model, ContextModel):
                    model.fit(train_contexts, train_labels)
                    oof_preds[val_idx, model_idx] = model.predict_proba(val_contexts)
                elif isinstance(model, KnowledgeCheckerModel):
                    model.fit()  # No training required
                    oof_preds[val_idx, model_idx] = model.predict_proba(val_texts)
                elif isinstance(model, VisionLanguageModel):
                    model.fit(train_texts, train_images, train_labels)
                    oof_preds[val_idx, model_idx] = model.predict_proba(val_texts, val_images)
                else:
                    raise ValueError(f"Unknown model type: {type(model)}")

            except Exception as e:
                print(f"    Error with {model_name}: {e}")
                # Fill with default predictions
                oof_preds[val_idx, model_idx] = 0.5

    return oof_preds


# ---------------------------------------------------------------------------
# MAIN PIPELINE
# ---------------------------------------------------------------------------

def train_and_evaluate(args):
    """Main training and evaluation pipeline."""
    print("Loading data...")
    train_df = pd.read_csv(args.train_csv)
    test_df = pd.read_csv(args.test_csv) if args.test_csv else None
    img_dir = None

    # Enable fast mode globally and optionally subsample
    global FAST_MODE
    FAST_MODE = bool(getattr(args, "fast", False))
    if FAST_MODE:
        # Subsample for speed
        train_df = train_df.sample(n=min(len(train_df), 1000), random_state=42)
        if test_df is not None and len(test_df) > 0:
            test_df = test_df.sample(n=min(len(test_df), 200), random_state=42)

    # Create datasets
    train_dataset = NewsDataset(train_df, img_dir)
    test_dataset = NewsDataset(test_df, img_dir) if test_df is not None else None

    # Make models directory
    models_dir = Path(__file__).parent / 'models'
    models_dir.mkdir(parents=True, exist_ok=True)

    print(f"Training samples: {len(train_dataset)}")
    if test_dataset:
        print(f"Test samples: {len(test_dataset)}")

    # Initialize base models
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Using device: {device}")

    models = [
        ContentModel(device=device),
        ContextModel(),
    ]
    if not getattr(args, 'no_nli', False):
        models.append(KnowledgeCheckerModel(device=device))

    print("Generating out-of-fold predictions for stacking...")
    folds = 2 if FAST_MODE else 5
    oof_predictions = generate_oof_predictions(models, train_dataset, n_folds=folds)

    print("Training meta-learner...")
    meta_learner = GradientBoostingClassifier(
        n_estimators=100,
        learning_rate=0.1,
        max_depth=3,
        random_state=42
    )
    meta_learner.fit(oof_predictions, train_dataset.labels)

    # Save artifacts
    print("Saving artifacts...")
    from datetime import datetime
    # Content model
    content = models[0]
    content_dir = models_dir / 'content'
    content_dir.mkdir(exist_ok=True)
    try:
        content.model.save_pretrained(str(content_dir))
        content.tokenizer.save_pretrained(str(content_dir))
    except Exception as e:
        print(f"Warning: failed to save content model: {e}")

    # Context model and scaler
    context = models[1]
    try:
        joblib.dump(context.model, models_dir / 'context_model.joblib')
        if hasattr(context, 'scaler'):
            joblib.dump(context.scaler, models_dir / 'context_scaler.joblib')
    except Exception as e:
        print(f"Warning: failed to save context artifacts: {e}")

    # Meta-learner and config
    try:
        joblib.dump(meta_learner, models_dir / 'meta_learner.joblib')
        # Save config with versioning and training meta
        cfg = {
            'used_nli': (not getattr(args, 'no_nli', False)),
            'fast_mode': FAST_MODE,
            'folds': folds,
            'trained_at': datetime.utcnow().isoformat() + 'Z',
            'version': datetime.utcnow().strftime('%Y%m%d%H%M%S'),
            'feature_order': ['content', 'context'] + (['nli'] if not getattr(args, 'no_nli', False) else []),
            'train_size': int(len(train_dataset)),
            'models': {
                'content_model': getattr(content, 'model_name', 'unknown'),
                'context_model': type(context.model).__name__
            }
        }
        (models_dir / 'config.json').write_text(json.dumps(cfg))
    except Exception as e:
        print(f"Warning: failed to save meta-learner/config: {e}")

    # Evaluate on training data (out-of-fold predictions)
    train_preds = meta_learner.predict(oof_predictions)
    train_probs = meta_learner.predict_proba(oof_predictions)[:, 1]

    print("\n=== Training Performance ===")
    print(f"Accuracy: {accuracy_score(train_dataset.labels, train_preds):.4f}")
    print(f"ROC-AUC: {roc_auc_score(train_dataset.labels, train_probs):.4f}")
    print("\nClassification Report:")
    print(classification_report(train_dataset.labels, train_preds,
                              target_names=['Real', 'Fake']))

    # Make predictions on test set if provided
    if test_dataset is not None:
        print("\nGenerating test predictions...")

        # Retrain all models on full training set
        train_texts = [sample.text for sample, _ in train_dataset]
        train_contexts = [sample.context_json for sample, _ in train_dataset]
        train_images = [sample.image_path for sample, _ in train_dataset]
        train_labels = train_dataset.labels

        test_texts = [sample.text for sample, _ in test_dataset]
        test_contexts = [sample.context_json for sample, _ in test_dataset]
        test_images = [sample.image_path for sample, _ in test_dataset]

        test_predictions = np.zeros((len(test_dataset), len(models)))

        for model_idx, model in enumerate(models):
            model_name = model.__class__.__name__
            print(f"  Retraining {model_name} on full training set...")

            try:
                if isinstance(model, ContentModel):
                    model.fit(train_texts, train_labels, epochs=2)
                    test_predictions[:, model_idx] = model.predict_proba(test_texts)
                elif isinstance(model, ContextModel):
                    model.fit(train_contexts, train_labels)
                    test_predictions[:, model_idx] = model.predict_proba(test_contexts)
                elif isinstance(model, KnowledgeCheckerModel):
                    model.fit()
                    test_predictions[:, model_idx] = model.predict_proba(test_texts)
                elif isinstance(model, VisionLanguageModel):
                    model.fit(train_texts, train_images, train_labels)
                    test_predictions[:, model_idx] = model.predict_proba(test_texts, test_images)
            except Exception as e:
                print(f"    Error with {model_name}: {e}")
                test_predictions[:, model_idx] = 0.5

        # Get final predictions from meta-learner
        final_predictions = meta_learner.predict(test_predictions)
        final_probabilities = meta_learner.predict_proba(test_predictions)[:, 1]

        # Save results
        results_df = test_df.copy()
        results_df['predicted_label'] = final_predictions
        results_df['fake_probability'] = final_probabilities

        output_path = args.output or 'results.csv'
        results_df.to_csv(output_path, index=False)
        print(f"\nResults saved to: {output_path}")

        # Show distribution of predictions
        fake_count = (final_predictions == 1).sum()
        real_count = (final_predictions == 0).sum()
        print(f"\nPrediction Distribution:")
        print(f"  Real: {real_count} ({real_count/len(final_predictions)*100:.1f}%)")
        print(f"  Fake: {fake_count} ({fake_count/len(final_predictions)*100:.1f}%)")


def main():
    parser = argparse.ArgumentParser(description="Stacked Fake News Detection Ensemble")
    parser.add_argument("--train_csv", type=str, required=True,
                       help="Path to training CSV file")
    parser.add_argument("--test_csv", type=str,
                       help="Path to test CSV file (optional)")
    parser.add_argument("--output", type=str, default="results.csv",
                       help="Output file for predictions")
    parser.add_argument("--fast", action="store_true",
                       help="Enable fast mode: subsample data and reduce sequence length / folds for a quick sanity run")
    parser.add_argument("--no_nli", action="store_true",
                       help="Disable KnowledgeCheckerModel (fast and avoids large NLI model)")

    args = parser.parse_args()
    train_and_evaluate(args)


# Jupyter/Colab friendly function
def run_fake_news_detection(train_csv_path, test_csv_path=None, img_dir_path="./images", output_path="results.csv"):
    """
    Notebook-friendly version of the fake news detection pipeline.

    Args:
        train_csv_path (str): Path to training CSV file
        test_csv_path (str, optional): Path to test CSV file
        img_dir_path (str): Directory containing images
        output_path (str): Output file for predictions
    """
    # Create a mock args object
    class Args:
        def __init__(self, train_csv, test_csv, img_dir, output):
            self.train_csv = train_csv
            self.test_csv = test_csv
            self.img_dir = img_dir
            self.output = output

    args = Args(train_csv_path, test_csv_path, img_dir_path, output_path)
    return train_and_evaluate(args)


# Example usage function for notebooks
def create_sample_data():
    """
    Create sample data files for testing the fake news detector.
    This is helpful for testing in Colab/Jupyter environments.
    """
    import os

    # Create sample training data
    sample_train_data = {
        'text': [
            'Scientists discover breakthrough in renewable energy technology',
            'Celebrity spotted with alien at local coffee shop says eyewitness',
            'New medical study shows promising results for cancer treatment',
            'Government officials confirm time travel experiments successful',
            'Local school wins national science competition',
            'Breaking: Moon made of cheese confirms NASA whistleblower'
        ],
        'label': [0, 1, 0, 1, 0, 1],  # 0 = real, 1 = fake
        'image_filename': ['img1.jpg', 'img2.jpg', 'img3.jpg', 'img4.jpg', 'img5.jpg', 'img6.jpg'],
        'context': [
            '{"share_count": 150, "unique_users": 45, "avg_followers": 1200, "burstiness": 0.3}',
            '{"share_count": 5000, "unique_users": 800, "avg_followers": 500, "burstiness": 0.9}',
            '{"share_count": 300, "unique_users": 120, "avg_followers": 2000, "burstiness": 0.2}',
            '{"share_count": 2000, "unique_users": 400, "avg_followers": 300, "burstiness": 0.8}',
            '{"share_count": 50, "unique_users": 25, "avg_followers": 800, "burstiness": 0.1}',
            '{"share_count": 10000, "unique_users": 1500, "avg_followers": 200, "burstiness": 0.95}'
        ]
    }

    # Create sample test data
    sample_test_data = {
        'text': [
            'Local mayor announces new infrastructure project',
            'Experts claim chocolate cures all diseases instantly'
        ],
        'image_filename': ['test_img1.jpg', 'test_img2.jpg'],
        'context': [
            '{"share_count": 75, "unique_users": 30, "avg_followers": 1000, "burstiness": 0.2}',
            '{"share_count": 3000, "unique_users": 600, "avg_followers": 400, "burstiness": 0.85}'
        ]
    }

    # Save to CSV files
    train_df = pd.DataFrame(sample_train_data)
    test_df = pd.DataFrame(sample_test_data)

    train_df.to_csv('sample_train.csv', index=False)
    test_df.to_csv('sample_test.csv', index=False)

    # Create images directory with dummy images
    os.makedirs('sample_images', exist_ok=True)

    # Create dummy images (small colored squares)
    from PIL import Image
    import random

    all_img_files = sample_train_data['image_filename'] + sample_test_data['image_filename']

    for img_file in all_img_files:
        # Create a small random colored image
        color = (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255))
        img = Image.new('RGB', (224, 224), color)
        img.save(f'sample_images/{img_file}')

    print("Sample data created:")
    print("- sample_train.csv (6 samples)")
    print("- sample_test.csv (2 samples)")
    print("- sample_images/ directory with dummy images")
    print("\nNow you can run:")
    print("run_fake_news_detection('sample_train.csv', 'sample_test.csv', 'sample_images')")


# Auto-detect environment and provide appropriate interface
if __name__ == "__main__":
    try:
        # Try to detect if we're in a notebook environment
        import sys
        if 'ipykernel' in sys.modules or 'google.colab' in sys.modules:
            print("=" * 60)
            print("NOTEBOOK ENVIRONMENT DETECTED")
            print("=" * 60)
            print("Use these functions instead of command-line arguments:")
            print()
            print("1. Create sample data:")
            print("   create_sample_data()")
            print()
            print("2. Run with sample data:")
            print("   run_fake_news_detection('sample_train.csv', 'sample_test.csv', 'sample_images')")
            print()
            print("3. Run with your own data:")
            print("   run_fake_news_detection('your_train.csv', 'your_test.csv', 'your_images_dir')")
            print()
            print("=" * 60)
        else:
            # Regular command-line execution
            main()
    except:
        # Fallback - assume command line
        main()